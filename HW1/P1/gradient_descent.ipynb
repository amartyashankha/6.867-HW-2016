{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import loadParametersP1\n",
    "import loadFittingDataP1\n",
    "from scipy.stats import multivariate_normal\n",
    "import pdb\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GD(object):\n",
    "    \n",
    "    def __init__(self, x0, objective,\n",
    "                 gradient=None,\n",
    "                 step_size=0.1):\n",
    "        self.x0 = x0\n",
    "        self.objective = objective\n",
    "        self.gradient = gradient\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def compute_gradient(self, x, idx=None, eps=1e-6):\n",
    "        if self.gradient != None:\n",
    "            return self.gradient(x)\n",
    "        grad = np.array([0.0 for i in range(len(x))])\n",
    "        if idx != None:\n",
    "            f_x = self.objective(x)\n",
    "            x[idx] += eps\n",
    "            f_eps = self.objective(x)\n",
    "            x[idx] -= eps\n",
    "            grad[idx] = (f_eps-f_x)/eps\n",
    "            return grad\n",
    "        X = copy.copy(x)\n",
    "        for i in range(len(x)):\n",
    "            f_x = self.objective(X)\n",
    "            #print X, X[i]+eps, self.objective(X), eps\n",
    "            X[i] = X[i] + eps\n",
    "            #print X, X[i], self.objective(X)\n",
    "            f_eps = self.objective(X)\n",
    "            X[i] = X[i] - eps\n",
    "            grad[i] = (f_eps-f_x)/eps\n",
    "        return grad\n",
    "        \n",
    "    \n",
    "    def step(self, stochastic=False, gtol=1e-8):\n",
    "        log = []\n",
    "        while True:\n",
    "            grad = self.compute_gradient(self.x0)\n",
    "            log.append((self.x0, self.objective(self.x0)))\n",
    "            if np.linalg.norm(grad) < gtol:\n",
    "                break\n",
    "            self.x0 = self.x0 - self.step_size * grad\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaussMean,gaussCov,quadBowlA,quadBowlb = loadParametersP1.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_gauss = lambda x : -multivariate_normal.pdf(x, gaussMean, gaussCov)\n",
    "test_gauss_gradient = lambda x : -test_gauss(x)*np.linalg.inv(gaussCov).dot(x-gaussMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_bowl = lambda x : 0.5*x.T.dot(quadBowlA.dot(x)) - x.T.dot(quadBowlb)\n",
    "test_bowl_gradient = lambda x : quadBowlA.dot(x) - quadBowlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = np.array([6.0, 16.0])\n",
    "log = GD(x0, test_gauss, None, 10000).step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([  9.9651509 ,  10.05227234]), -0.00015915462901127802), 2987)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[-1], len(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y = loadFittingDataP1.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    \n",
    "    def __init__(self, X, y,\n",
    "                 step_size=1e-5):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.step_size = step_size\n",
    "    \n",
    "    def compute_objective(self, theta):\n",
    "        return sum([(theta.dot(X[i])-y[i])**2 for i in range(len(y))])\n",
    "    \n",
    "    def compute_numerical_gradient(self, theta, eps=1e-7, idx=None):\n",
    "        grad = np.zeros(len(theta))\n",
    "        for i in range(len(theta)):\n",
    "            if idx != None and i != idx:\n",
    "                continue\n",
    "            f = self.compute_objective(theta)\n",
    "            theta[i] = theta[i] + eps\n",
    "            f_eps = self.compute_objective(theta)\n",
    "            theta[i] = theta[i] - eps\n",
    "            grad[i] = (f_eps-f)/eps\n",
    "        return grad\n",
    "        \n",
    "    def compute_gradient(self, theta, idx=None):\n",
    "        if idx == None:\n",
    "            idx = range(len(self.y))\n",
    "        grad = np.zeros(self.X.shape[1])\n",
    "        for i in idx:\n",
    "            grad += 2 * (self.X[i].dot(theta) - self.y[i]) * self.X[i]\n",
    "        return grad\n",
    "    \n",
    "    def step(self, theta, stochastic=False, minibatch_size=1, ftol=1e-7):\n",
    "        log = []\n",
    "        idx = None\n",
    "        prev_objective = self.compute_objective(theta)\n",
    "        t_0 = 0.01\n",
    "        t = 0\n",
    "        k = 0.7\n",
    "        while True:\n",
    "            if stochastic:\n",
    "                idx = np.random.randint(self.X.shape[1], size=minibatch_size)\n",
    "            grad = self.compute_gradient(theta)\n",
    "            log.append((theta, prev_objective))\n",
    "            step_size = math.pow(t_0+t, -k)\n",
    "            theta = theta - self.step_size * grad\n",
    "            tmp = self.compute_objective(theta)\n",
    "            if abs(tmp-prev_objective) < ftol:\n",
    "                break\n",
    "            prev_objective = tmp\n",
    "            t += 1\n",
    "            print log[-1][1], grad\n",
    "        return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = SGD(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02228589  0.05137948  0.08752402 -0.08297291  0.12458932  0.07481222\n",
      " -0.06388252 -0.04999496  0.01166251  0.02678426]\n"
     ]
    }
   ],
   "source": [
    "theta_0 = np.random.random(X.shape[1])\n",
    "print optimizer.compute_gradient(theta_0) - optimizer.compute_numerical_gradient(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20773371.7734 [   -2451.02243875    85364.25453889   577638.18341395  -280364.55387158\n",
      "   222822.89750581  -280368.6515151     17711.8464542    107290.23670455\n",
      "  1194392.33732171 -1177688.07827311]\n",
      "1426781.1278 [ -47464.54265331  143450.7692434    33924.78356994 -230658.32004675\n",
      "  -43878.44362921 -149890.63536861 -146200.47028245   18951.84979319\n",
      "   71075.15210149 -240251.68661893]\n",
      "215771.750315 [ 47146.71496224  37920.70341357  43496.50073172 -44877.47793889\n",
      "   2548.86543549 -82041.08907787 -53640.80615243   5423.74939752\n",
      "  22833.89312737 -80480.57171984]\n",
      "49731.8569043 [  4089.97902502  13714.81106705  12924.58616518 -29349.85232198\n",
      "   1369.77920248 -34767.24014152 -36227.84827368 -14952.18338966\n",
      "   7843.73710467 -18754.79461546]\n",
      "18203.0890452 [ 11560.90023762   3359.19869084   8812.75270338  -7722.10797995\n",
      "   1947.93953803 -16673.81101444 -18242.7747863   -4839.88712367\n",
      "   3124.18157031  -7385.01654175]\n",
      "10916.5041462 [  3154.50277586    557.86467489   3431.63292228  -5251.68050596\n",
      "    526.99691079  -7725.59171064 -10750.21707522  -4233.98016973\n",
      "   1620.83201877  -1638.42790363]\n",
      "9044.69990259 [ 2807.37051402  -363.93837415  2116.53497536 -1864.06282381   253.3068654\n",
      " -3900.95909713 -5650.45273453 -1599.87244559   822.13231918  -701.02086072]\n",
      "8535.23786904 [  1.06364283e+03  -4.26762571e+02   9.85959898e+02  -1.15215906e+03\n",
      "   2.86456003e+00  -1.94170721e+03  -3.18738341e+03  -9.83447046e+02\n",
      "   4.58368346e+02  -1.09105150e+02]\n",
      "8391.71599792 [  696.84314767  -347.53966066   574.57242579  -487.92166734   -29.16829997\n",
      " -1013.91533537 -1715.22001957  -407.40938918   247.68126043   -40.53954286]\n",
      "8350.38083217 [ 308.07496788 -231.78653241  293.60384318 -275.9687544   -45.25885178\n",
      " -526.72500132 -952.86237127 -222.3300729   138.36184666   13.41377145]\n",
      "8338.29794644 [ 178.2307391  -147.02484855  165.93355187 -127.58104831  -34.27694663\n",
      " -280.58640371 -519.0132854   -98.63308919   76.00890748   10.43098744]\n",
      "8334.72950044 [  85.53188285  -88.26746284   88.41884527  -68.35523103  -25.27240764\n",
      " -149.34759948 -286.74073696  -51.37559135   42.30000022   10.77734992]\n",
      "8333.66797704 [  46.91325982  -52.01225367   49.23759708  -32.98500343  -16.41049472\n",
      "  -80.53496869 -157.11601562  -24.03022662   23.36130715    6.53342925]\n",
      "8333.35055848 [ 23.56653395 -29.97283967  26.72268838 -17.1633161  -10.50048049\n",
      " -43.46443577 -86.63458117 -12.33762379  12.96954827   4.34966737]\n",
      "8333.25528429 [ 12.65311081 -17.11267966  14.7838465   -8.46525406  -6.41919375\n",
      " -23.61906627 -47.6078408   -6.00934337   7.17550298   2.51635162]\n",
      "8333.22660749 [  6.52729771  -9.66773007   8.08899124  -4.34129076  -3.88002126\n",
      " -12.85036849 -26.23546483  -3.08856092   3.97793516   1.5023686 ]\n",
      "8333.21795792 [  3.48183273  -5.43364293   4.46304891  -2.16747705  -2.29603041\n",
      "  -7.01747849 -14.43757289  -1.55098253   2.20160441   0.85104648]\n",
      "8333.21534487 [ 1.8257913  -3.037396    2.45095572 -1.1047879  -1.34714028 -3.83635277\n",
      " -7.95519914 -0.8034081   1.21928976  0.48718181]\n",
      "8333.21455451 [ 0.9741116  -1.69282141  1.35093639 -0.55596596 -0.78130149 -2.10163295\n",
      " -4.38094758 -0.41271797  0.67465515  0.27274909]\n",
      "8333.21431522 [ 0.51638138 -0.94069531  0.74316911 -0.2830332  -0.45027299 -1.15228425\n",
      " -2.41400193 -0.21591851  0.37333314  0.1531763 ]\n",
      "8333.21424272 [ 0.2763263  -0.52181713  0.40949677 -0.14331611 -0.2576676  -0.6325317\n",
      " -1.32989264 -0.11280189  0.20647465  0.08516315]\n",
      "8333.21422075 [ 0.14762681 -0.2889881   0.22546007 -0.07308505 -0.14676174 -0.34743003\n",
      " -0.73284783 -0.05958701  0.11417777  0.04737879]\n",
      "8333.21421408 [ 0.07930696 -0.15987657  0.12422551 -0.03721861 -0.08320208 -0.19096874\n",
      " -0.40381272 -0.03152129  0.06311394  0.02623382]\n",
      "8333.21421206 [ 0.04262182 -0.08836662  0.06842558 -0.01904791 -0.04700542 -0.10501225\n",
      " -0.22253751 -0.0167923   0.0348806   0.01452421]\n",
      "8333.21421145 [ 0.02298782 -0.04881125  0.03770303 -0.00975608 -0.02646939 -0.05777078\n",
      " -0.12263606 -0.00896639  0.01927141  0.00802295]\n"
     ]
    }
   ],
   "source": [
    "optimizer.step(np.random.random(X.shape[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10] 5.000001 1e-06\n",
      "[5.000001, 10] 5.000001\n"
     ]
    }
   ],
   "source": [
    "x = [5, 10]\n",
    "eps = 1e-6\n",
    "i=0\n",
    "print x, x[i]+eps, eps\n",
    "x[i] = x[i] + eps\n",
    "print x, x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.6 (default, Jun 22 2015, 17:58:13) \n",
      "[GCC 4.8.2]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
